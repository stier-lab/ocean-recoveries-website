{
  "filename": "White et al. (Oikos) 2014.pdf",
  "numPages": 4,
  "text": "385 Ecologists should not use statistical significance tests to interpret simulation model results J. Wilson White, Andrew Rassweiler, Jameal F. Samhouri, Adrian C. Stier and Crow White J. W. White (whitejw@uncw.edu), Dept of Biology and Marine Biology, Univ. of North Carolina Wilmington, Wilmington, NC 28403, USA. – A. Rassweiler, Marine Science Inst., Univ. of California Santa Barbara, Santa Barbara, CA 93106, USA. – J. F. Samhouri, Conservation Biology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, Seattle, WA 98112, USA. – A. C. Stier, Dept of Zoology, Univ. of British Columbia, Vancouver, BC, V6T 1Z4, Canada. – C. White, Dept of Biological Sciences, California Polytechnic State Univ., San Luis Obispo, CA 93407, USA. Simulation models are widely used to represent the dynamics of ecological systems. A common question with such models is how changes to a parameter value or functional form in the model alter the results. Some authors have chosen to answer that question using frequentist statistical hypothesis tests (e.g. ANOVA). This is inappropriate for two reasons. First, p-values are determined by statistical power (i.e. replication), which can be arbitrarily high in a simulation context, producing minuscule p-values regardless of the effect size. Second, the null hypothesis of no difference between treatments (e.g. parameter values) is known a priori to be false, invalidating the premise of the test. Use of p-values is troublesome (rather than simply irrelevant) because small p-values lend a false sense of importance to observed differences. We argue that modelers should abandon this practice and focus on evaluating the magnitude of differences between simulations. A growing number of authors in the ecological literature use statistical methods common to experimental ecology to analyze the output of ecological simulation models. For example, authors may use analysis of variance (ANOVA) to test whether model runs with different parameter values or different functional forms produce statistically different outputs. We view significance testing applied to simulation model output as a misuse of statistical theory. In this article we explain our reasoning with the goals of discouraging the practice, encouraging instead a focus on the magnitude of differences between simulations (i.e. effect sizes), and sparking discussion regarding when – if ever – statistical sig- nificance tests could be appropriate. The perils of placing too much emphasis on statistical tests are well known in ecology. The past few decades have seen several essays reminding ecologists not to con- flate statistical with biological significance, that p-values are essentially arbitrary thresholds, and that p-values are meaningless unless accompanied by measures of effect size and statistical power (Yoccoz 1991, Johnson 1999, Hurlbert and Lombardi 2009, Beninger et al. 2012). The use of frequentist statistical tests in a simulation model set- ting presents two additional issues, the first practical and the second philosophical: 1) Statistical power is determined by replication (Berkson 1938), which is a trivial notion in the era of modern computing. Therefore power, and thus p-values, are determined only by the number of simulations one chooses to run. 2) The ‘truth’ of a testable null hypothesis is assumed to be unknown. In a model context, the programmer knows the ‘truth’ (because they know the model parameters) and testing a known-to-be-false null hypothesis does not provide useful information (Savage 1957, Johnson 1999, Anderson et al. 2000). While we contend that a focus on statistical significance is inappropriate, we also argue that quantitative evaluation of differences in effect size among model scenarios is entirely Oikos 123: 385–388, 2014 doi: 10.1111/j.1600-0706.2013.01073.x Published 2013. This article is a US Government work and is in the public domain in the USA. Oikos © 2013 Nordic Society Oikos Subject Editor: Dustin Marshall. Accepted 11 October 2013 C h o i c e E d i t o r ’ s OIKOS Researchers analyzing field or lab data often test ecological hypotheses using frequentist statistics (t-tests, ANOVA, etc.) that focus on p-values. Field and lab data usually have limited sample sizes, and p-values are valuable for quantifying the probability of making incorrect inferences in that situation. However, modern ecologists increasingly rely on simulation models to address complex questions, and those who were trained in frequentist statistics often apply the hypothesis-testing approach inappropriately to their simulation results. Our paper explains why p-values are not informative for interpreting simulation models, and suggests better ways to evaluate the ecological significance of model results. Synthesis 386 appropriate. Moreover, in some cases frequentist statistical models (e.g. regression, ANOVA) may provide accessible and simple methods for quantifying effect sizes. Though the distinction between statistical and biological significance (Yoccoz 1991) may at first appear subtle, it is critical. Below we use a few recent examples from the ecological lit- erature to illustrate the context in which these issues arise, highlight potential pitfalls in the analysis and interpretation of simulation model output, and offer ways forward for the specific examples mentioned here and for analysis and interpretation of simulation model output in general. MANOVA with n   24 000 Marzloff et al. (2013) present a validation and sensitivity analysis of TRITON, a simulation model of alternative-state dynamics in temperate rocky reef communities that centers on trophic interactions between lobsters Jasus edwardsii, sea urchins Centrostephanus rodgersii and seaweeds Ecklonia radiata and Phyllospora comosa. As part of their sensitivity analysis they compared the effects of alternative formulations of the lobster predatory functional response (Holling type I, II or III). To do so they performed 8000 simulations with random initial conditions and used MANOVA to compare the effects of functional response (the fixed effect) on a multivariate index of community state. They found that the form of the functional response had a highly significant effect in the MANOVA (p 10  15 , F 2,23997   67.5), but conceded that of course this significance was due to having nearly 24 000 denominator degrees of freedom in the F-test. They therefore ignored the MANOVA results and simply com- pared model results visually, concluding that the predator functional response had little effect on community state despite the exceedingly low p-value. This example nicely illustrates the two key problems we have identified. The question Marzloff et al. (2013) were attempting to answer (does the lobster functional response affect the predictions of their model regarding community state?) was not suited for null hypothesis testing. We know a priori from analytical models that different forms of the predator functional response produce distinct dynamics (Oaten and Murdoch 1975a, b), so the question is not whether the model outcomes will be different, but rather how different they will be. Even if we did not have the advantage of prior knowledge about the dynamical conse- quences of differences between functional responses, it would still be redundant to test for those differences using a null hypothesis framework. The null hypothesis is implic- itly posed as follows: ‘model results using each of the three functional responses are drawn from populations with identical distributions’. Because we know the model was programmed with different functions and parameters in each case, we also know a priori that the null hypothesis is false. Thus any failure to reject the null hypothesis is by definition a type-II error, and the test merely needs suffi- cient power to detect an effect and avoid that error. The second problem with hypothesis tests like the one Marzloff et al. (2013) perform is the arbitrariness of statisti- cal power itself in this setting. A sample size of 24,000 will generally produce a significant result (p 0.05, or even much lower) regardless of the magnitude of the biological effect size. Indeed, when dealing with this type of model, one can literally choose the desired p-value by setting the number of runs. With sufficient computer time, there is no limit to how small a value can be obtained. This sample size influence occurs, of course, in empirical experiments, not just computer simulations: any effect size, no matter how small, can be found significant if one is able to obtain enough replicates (Berkson 1938). However the ease and extremely low cost of replication in a simulation model set- ting represents an absurd extreme for this general principle. Marzloff et al. (2013) were correct to disregard the low p-values in their MANOVA, although we disagree with their choice to fall back on a qualitative visual inspection of the model output. A better alternative would have been to directly quantify the effect size in the MANOVA (or simply compare the distributions of community states; in their case a geomet- ric comparison of the positions of model solutions in principal components space), having determined beforehand what magnitude of difference (perhaps a percentage difference from some baseline) would constitute a significant ecological effect in the study system. This is analogous to the argument that ecologists should focus on ‘biological significance’ rather than statistical significance (Berkson 1942, Yoccoz 1991). Ecological simulation models are not the only context in which one could argue that the null hypothesis is known to be false a priori. Johnson (1999) catalogues several trivial null hypotheses that were rejected in empirical field studies (e.g. that logged and unlogged forests have the same den- sity of trees). Johnson (1999) and others have argued that null hypotheses are usually false in observational studies, such as when comparing some variable (e.g. fish abun- dance) among two locations; it is unthinkable that the two locations would have precisely identical population statis- tics. This situation stands in contrast to the case of a con- trolled ecological experiment, in which experimental replicates are randomly assigned to treatments, and a null hypothesis of no difference among the treatment popula- tions is actually a reasonable expectation (Johnson 1999). Because simulation models are in a sense controlled numer- ical experiments it is tempting to analyze their output as one would an empirical experiment. However, in simula- tion models the relationship between treatment (parameter value, functional form) and some dependent response (model output) is explicit (mechanistic) and known (or at least understood to exist); the question is simply how many replicates are needed to detect that relationship statistically. In empirical ecology, we accept that there is usually some correspondence between statistical significance and bio- logical significance (i.e. effect size); replication is difficult and costly to obtain, so power is a limiting factor and if a statistically significant signal is detected, it is likely to be biologically meaningful as well. That correspondence breaks down with the immense replication and power available in a simulation model setting. Indeed, it is these features of models that enable their analysis to bypass significance- tests and focus on the key result of interest, effect size. Are peer reviewers the problem? We u s e d Ma r z l o ff e t al. (2013) as an instructive example of the folly of null hypothesis tests in part because the authors 387 themselves admitted that the p-values they report are essen- tially meaningless. Indeed, the halfhearted use of MANOVA in their paper gives the sense of a test that was demanded by a reviewer during the peer review process (after all, if the test results are meaningless, why bother conducting the test at all, or taking up space in the paper to report it?). We have encountered similar pressure from reviewers in our own work, and worry that it may be a common occurrence. A non-systematic search quickly revealed many recent examples spanning the breadth of the ecological literature, from terrestrial plant ecology to fisheries to life-history theory (Marshall et al. 2008, Dauphin et al. 2009, Esther et al. 2010, Makler-Pick et al. 2011, Tam and Ang 2012). We suspect that calculating p-values for simulation model results is actually more commonplace than a literature search would reveal, because doubtless there are many instances of authors attempting this practice, but having it stripped out in review (which all of the authors of this paper have done as reviewer). We have also experienced the opposite interaction, in which reviewer advocate for the necessity of p-values. For example, Samhouri et al. (2009) analyzed seven different Ecopath with Ecosim models of marine food web dynamics to identify system-specific indi- cators (i.e. biomass of a particular functional group) that reveal changes to key ecosystem attributes (i.e. emergent properties such as diversity, net primary productivity, mean trophic level, etc.). For each model, they simulated a range of fishing perturbations to the model ecosystem, recorded the responses of a suite of indicators and attributes, and calculated the correlations between each indicator and attribute. In their paper, they reported only the magnitude of the correlation coefficients, not their statistical signifi- cance, noting that because of their large sample size, even small, biologically unmeaningful correlations would be sta- tistically significant (also see Fulton et al. 2005). This omis- sion was a point of contention during review, as a reviewer focused on the potential for reporting spurious correlations in the absence of p-values. Indeed, there are plenty of examples in the literature in which p-values are reported to support or refute the use of indicators (Travers et al. 2006). Samhouri et al. (2009) won the argument with their reviewer, but we wonder how frequently well-intentioned reviewers – particularly those with more experience with empirical data than models – insist on inappropriate hypothesis tests for simulation data. The importance of p-values and rigorous hypothesis tests is drilled into bud- ding ecologists early and often, so the mistake is under- standable. We hope that this paper helps formalize the argument for modelers, and clarify the understanding of reviewers, of the use and misuse of statistical tests in modeling studies. Is there any use for frequentist statistics in ecological simulation models? We have argued that frequentists tests of null hypotheses are useless when comparing model simulation output with dif- ferent parameter values or functional forms. However, there are situations in which statistical tools associated with fre- quentist significance testing can be used productively in concert with simulation models. For example, methods such as ANOVA provide a useful and familiar framework for partitioning variance and calculating effect sizes in mul- tifactorial simulations. It is appropriate to use ANOVA in that way, provided one ignores (and does not report) the p-values calculated along the way. Corell et al. (2012) provide a representative example of this usage. They examined the factors affecting dispersal dis- tances of planktonic marine larvae in the Baltic Sea. They used a numerical hydrodynamic circulation model to describe three-dimensional flow field of ocean currents in the study region, and conducted simulations in which they released simulated Lagrangian particles (virtual ‘larvae’) into the flow field and observed their trajectories. Larval trajecto- ries were affected by random turbulence, so the dispersal model includes stochasticity. The authors examined the effects of multiple factors on dispersal trajectories (e.g. spawning season, larval depth, duration of the larval period, etc.) and created a factorial design to examine each of 216 individual treatment combinations, with three replicate simulations per combination. They then analyzed their results using a five-way ANOVA. However, rather than use ANOVA to test the hypothesis that different parameter val- ues produce different dispersal patterns (which is of course true), they focused on the variance components returned by ANOVA in order to determine which factor contributed to greater absolute variation in dispersal dis- tance. This type of usage is perfectly reasonable (see also Legendre and De Cáceres 2013). Similar applications of statistical methods could be, for example, using a Komolgorov–Smirnov test to compare two distributions generated by data; again, however, one should focus on the effect size or test statistic itself, rather than the p-value. Another context in which hypothesis testing is usefully applied to simulation results is when one desires to simu- late the empirical measurement of a system. This might be done to test alternative statistical or experimental approaches in a system with known dynamics, or to determine how the output of a simulated process compares to observed data. In the first case, models are used to simulate both process and measurement error, and model analysis focuses on determining the level of empirical replication needed to detect a process (Hoban et al. 2012) or validating a new statistical method for detecting certain phenomena (Dakos et al. 2012). These studies are essentially statistical power analyses in which the known falsehood of the null hypoth- esis is taken as a given. An additional case where statistics could appropriately be applied to simulation outputs is when simulation results are being compared to observed empirical data. For exam- ple, Walker and Cyr (2007) simulated neutral community dynamics and used statistics to determine whether those dynamics matched observed species abundance distribu- tions. Similar comparisons are common in time series applications where model forecast skill is the statistic of interest (Sugihara 1994). Although the details of compar- ing simulated results to real data is a separate topic that is beyond the scope of our commentary, we note that stan- dard frequentist approaches are also perilous in that con- text (Waller et al. 2003), and newer methods such as Approximate Bayesian Computation are more reliable (Hoban et al. 2012). 388 Dauphin, G. et al. 2009. Host kairomone learning and foraging success in an egg parasitoid: a simulation model. – Ecol. Entomol. 34: 193–203. Esther, A. et al. 2010. Sensitivity of plant functional types to climate change: classification tree analysis of a simulation model. – J. Veg. Sci. 21: 447–461. Fulton, E. A. et al. 2005. Which ecological indicators can robustly detect effects of fishing? – ICES J. Mar. Sci. 62: 540–551. Hoban, S. et al. 2012. Computer simulations: tools for population and evolutionary genetics. – Nat. Rev. 13: 110–113. Hurlbert, S. H. and Lombardi, C. M. 2009. Final collapse of the Neyman–Pearson decision theoretic framework and rise of the neoFisherian. – Ann. Zool. Fenn. 46: 311–349. Johnson, D. H. 1999. The insignificance of statistical significance testing. – J. Wildlife Manage. 63: 763–772. Legendre, P. and De Cáceres, M. 2013. Beta diversity as the variance of community data: dissimilarity coefficients and partitioning. – Ecol. Lett. 16: 951–963. Makler-Pick, V. et al. 2011. Exploring the role of fish in a lake ecosystem (Lake Kinneret, Isreal) by coupling an individual- based fish population model to a dynamic ecosystem model. – Can. J. Fish. Aquat. Sci. 68: 1265–1284. Marshall, D. J. et al. 2008. Offspring size variation within broods as a bet-hedging strategy in unpredictable environments. – Ecology 89: 2506–2517. Marzloff, M. P. et al. 2013. Sensitivity analysis and pattern-oriented validation of TRITON, a model with alternative community states: insights on temperate rocky reefs dynamics. – Ecol. Modell. 258: 16–32. Oaten, A. and Murdoch, W. W. 1975a. Functional response and stability in predator–prey systems. – Am. Nat. 109: 289–298. Oaten, A. and Murdoch, W. W. 1975b. Switching, functional response, and stability in predator–prey systems. – Am. Nat. 109: 299–318. Samhouri, J. F. et al. 2009. Quantitative evaluation of marine ecosystem indicator performance using food web models. – Ecosystems 12: 1283–1298. Savage, I. R. 1957. Nonparametric statistics. – J. Am. Stat. Ass. 52: 331–344. Sugihara, G. 1994. Nonlinear forecasting for the classification of natural time series. – Phil. Trans. R. Soc. A 348: 477–495. Tam, T. and Ang, P. O. 2012. Object-oriented simulation of coral competition in a coral reef community. – Ecol. Modell. 245: 111–120. Tr a v e r s , M . e t al. 2006. Simulating and testing the sensitivity of ecosystem-based indicators to fishing in the southern Benguela ecosystem. – Can. J. Fish. Aquat. Sci. 63: 943–956. Walker, S. C. and Cyr, H. 2007. Testing the standard neutral model of biodiversity in lake communities. – Oikos 116: 143–155. Waller, L. A. et al. 2003. Monte Carlo assessments of goodness- of-fit for ecological simulation models. – Ecol. Modell. 164: 49–63. Yoccoz, N. G. 1991. Use, overuse, and misuse of significance tests in evolutionary biology and ecology. – Bull. Ecol. Soc. Am. 72: 106–111. Conclusion Ecologists increasingly appreciate the importance of stochastic variability, spatiotemporal heterogeneity, and complex non- linearities to the dynamics of natural systems (Comins et al. 1992, Anderson et al. 2008, Berkley et al. 2010). This realization – coupled with the availability of fast computers – is motivating increasing reliance upon large simulation models. Here we have explained why it is inappropriate to rely upon frequentist statistical hypothesis tests designed for low-replication empirical experiments when comparing highly replicated outputs of simulation models. We have also pointed out a few suitable applications of frequentist statis- tics to simulation model output, and there are surely others. Nonetheless, we call upon authors (and reviewers) to avoid the temptation to analyze model output with a focus on statistical significance. The key insights to be to gleaned from simulation models, as with empirical data, must come from interpretation of biological significance. Acknowledgements – Authorship statement: all authors designed the study; JWW wrote the manuscript with input from all authors. AR, JFS, ACS and CW contributed equally and their author order is alphabetical. JWW was supported by Florida Sea Grant PD/12-18. ACS was supported by a Killam Fellowship. We thank the Subject Editor for suggestions that improved this manuscript. We also respectfully acknowledge the authors whose works we singled out for discussion. We thank O. Shelton, M. Mangel and the Subject Editor for suggestions that improved the manuscript. References Anderson, C. N. K. et al. 2008. Why fishing magnifies fluctuations in fish abundance. – Nature 452: 835–839. Anderson, D. R. et al. 2000. Null hypothesis testing: problems, prevalence and an alternative. – J. Wildlife Manage. 64: 912–923. Beninger, P. G. et al. 2012. Strengthening statistical usage in marine ecology. – J. Exp. Mar. Biol. Ecol. 426–427: 97–108. Berkley, H. A. et al. 2010. Turbulent dispersal promotes species coexistence. – Ecol. Lett. 13: 360–371. Berkson, J. 1938. Some difficulties of interpretation encountered in the application of the chi-square test. – J. Am. Stat. Ass. 33: 526–542. Berkson, J. 1942. Tests of significance considered as evidence. – J. Am. Stat. Ass. 37: 325–335. Comins, H. N. et al. 1992. The spatial dynamics of host–parasitoid systems. – J. Anim. Ecol. 61: 735–748. Corell, H. et al. 2012. Depth distribution of larvae critically affects their dispersal and the efficiency of marine protected areas. – Mar. Ecol. Prog. Ser. 467: 29–46. Dakos, V. et al. 2012. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. – PLoS ONE 7:e41010.",
  "sections": {
    "abstract": "",
    "introduction": "",
    "methods": "",
    "results": "",
    "discussion": "",
    "conclusion": ""
  },
  "keyFindings": [
    "significant effect in the MANOVA (p 10  15 , F 2,23997   67."
  ],
  "fullText": "\n\n385\nEcologists should not use statistical significance tests to interpret \nsimulation model results\nJ. Wilson White, Andrew Rassweiler, Jameal F. Samhouri, Adrian C. Stier and Crow White\nJ. W. White (whitejw@uncw.edu), Dept of Biology and Marine Biology, Univ. of North Carolina Wilmington, Wilmington, NC 28403,  \nUSA. – A. Rassweiler, Marine Science Inst., Univ. of California Santa Barbara, Santa Barbara, CA 93106, USA. – J. F. Samhouri, \nConservation Biology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric \nAdministration, Seattle, WA 98112, USA. – A. C. Stier, Dept of Zoology, Univ. of British Columbia, Vancouver, BC, V6T 1Z4,  \nCanada. – C. White, Dept of Biological Sciences, California Polytechnic State Univ., San Luis Obispo, CA 93407, USA.\nSimulation models are widely used to represent the dynamics of ecological systems. A common question with such models \nis  how  changes  to  a  parameter  value  or  functional  form  in  the  model  alter  the  results.  Some  authors  have  chosen  to \nanswer  that  question  using  frequentist  statistical  hypothesis  tests  (e.g.  ANOVA).  This  is  inappropriate  for  two  reasons. \nFirst, p-values are determined by statistical power (i.e. replication), which can be arbitrarily high in a simulation context, \nproducing minuscule p-values regardless of the effect size. Second, the null hypothesis of no difference between treatments \n(e.g. parameter values) is known a priori to be false, invalidating the premise of the test. Use of p-values is troublesome \n(rather than simply irrelevant) because small p-values lend a false sense of importance to observed differences. We argue \nthat modelers should abandon this practice and focus on evaluating the magnitude of differences between simulations.\nA  growing  number  of  authors  in  the  ecological  literature  \nuse statistical methods common to experimental ecology to \nanalyze  the  output  of  ecological  simulation  models.  For \nexample, authors may use analysis of variance (ANOVA) to \ntest  whether  model  runs  with  different  parameter  values  \nor  different  functional  forms  produce  statistically  different \noutputs. We view significance testing applied to simulation \nmodel output as a misuse of statistical theory. In this article \nwe  explain  our  reasoning  with  the  goals  of  discouraging  \nthe practice, encouraging instead a focus on the magnitude \nof  differences  between  simulations  (i.e.  effect  sizes),  and \nsparking discussion regarding when – if ever – statistical sig-\nnificance tests could be appropriate.\nThe  perils  of  placing  too  much  emphasis  on  statistical \ntests  are  well  known  in  ecology.  The  past  few  decades  \nhave  seen  several  essays  reminding  ecologists  not  to  con-\nflate  statistical  with  biological  significance,  that  p-values \nare  essentially  arbitrary  thresholds,  and  that  p-values  are \nmeaningless  unless  accompanied  by  measures  of  effect  \nsize  and  statistical  power  (Yoccoz  1991,  Johnson  1999, \nHurlbert  and  Lombardi  2009,  Beninger  et al.  2012).  The \nuse of frequentist statistical tests in a simulation model set-\nting  presents  two  additional  issues,  the  first  practical  and \nthe second philosophical:\n1) Statistical  power  is  determined  by  replication  (Berkson  \n1938),  which  is  a  trivial  notion  in  the  era  of  modern \ncomputing.  Therefore  power,  and  thus  p-values,  are  \ndetermined  only  by  the  number  of  simulations  one \nchooses to run.\n2) The ‘truth’ of a testable null hypothesis is assumed to be \nunknown.  In  a  model  context,  the  programmer  knows \nthe  ‘truth’  (because  they  know  the  model  parameters) \nand testing a known-to-be-false null hypothesis does not \nprovide useful information (Savage 1957, Johnson 1999, \nAnderson et al. 2000).\nWhile  we  contend  that  a  focus  on  statistical  significance  is \ninappropriate, we also argue that quantitative evaluation of \ndifferences  in  effect  size  among  model  scenarios  is  entirely \nOikos 123: 385–388, 2014 \ndoi: 10.1111/j.1600-0706.2013.01073.x\nPublished 2013. This article is a US Government work and is in the public domain in the USA. \nOikos © 2013 Nordic Society Oikos\nSubject Editor: Dustin Marshall. Accepted 11 October 2013\nC\nh\no\ni\nc\ne\nE\nd\ni\nt\no\nr\n’\ns\nOIKOS\nResearchers  analyzing  field  or  lab  data  often  test  ecological  hypotheses  using  frequentist  statistics  (t-tests,  \nANOVA, etc.) that focus on p-values. Field and lab data usually have limited sample sizes, and p-values are \nvaluable  for  quantifying  the  probability  of  making  incorrect  inferences  in  that  situation.    However,  modern \necologists increasingly rely on simulation models to address complex questions, and those who were trained in \nfrequentist statistics often apply the hypothesis-testing approach inappropriately to their simulation results. Our \npaper explains why p-values are not informative for interpreting simulation models, and suggests better ways to \nevaluate the ecological significance of model results.\nSynthesis\n\n386\nappropriate.  Moreover,  in  some  cases  frequentist  statistical \nmodels  (e.g.  regression,  ANOVA)  may  provide  accessible \nand  simple  methods  for  quantifying  effect  sizes.  Though  \nthe distinction between statistical and biological significance \n(Yoccoz  1991)  may  at  first  appear  subtle,  it  is  critical.  \nBelow we use a few recent examples from the ecological lit-\nerature  to  illustrate  the  context  in  which  these  issues  arise, \nhighlight potential pitfalls in the analysis and interpretation \nof  simulation  model  output,  and  offer  ways  forward  for  \nthe  specific  examples  mentioned  here  and  for  analysis  and \ninterpretation of simulation model output in general.\nMANOVA with n \u0015 24 000\nMarzloff  et al.  (2013)  present  a  validation  and  sensitivity \nanalysis of TRITON, a simulation model of alternative-state \ndynamics in temperate rocky reef communities that centers \non trophic interactions between lobsters Jasus edwardsii, sea \nurchins Centrostephanus  rodgersii  and  seaweeds Ecklonia  \nradiata  and Phyllospora  comosa.  As  part  of  their  sensitivity \nanalysis they compared the effects of alternative formulations \nof the lobster predatory functional response (Holling type I, \nII  or  III). To  do  so  they  performed  8000  simulations  with \nrandom initial conditions and used MANOVA to compare \nthe  effects  of  functional  response  (the  fixed  effect)  on  a  \nmultivariate index of community state. They found that the \nform of the functional response had a highly significant effect \nin the MANOVA (p \f 10\n\u001215\n, F\n2,23997\n \u0015 67.5), but conceded \nthat  of  course  this  significance  was  due  to  having  nearly  \n24 000 denominator degrees of freedom in the F-test. They \ntherefore  ignored  the  MANOVA  results  and  simply  com-\npared  model  results  visually,  concluding  that  the  predator \nfunctional  response  had  little  effect  on  community  state \ndespite the exceedingly low p-value.\nThis example nicely illustrates the two key problems we \nhave  identified.  The  question  Marzloff  et al.  (2013)  were \nattempting to answer (does the lobster functional response \naffect the predictions of their model regarding community \nstate?) was not suited for null hypothesis testing. We know \na priori from analytical models that different forms of the \npredator  functional  response  produce  distinct  dynamics \n(Oaten  and  Murdoch  1975a,  b),  so  the  question  is  not \nwhether  the  model  outcomes  will  be  different,  but  rather \nhow  different  they  will  be.  Even  if  we  did  not  have  the \nadvantage of prior knowledge about the dynamical conse-\nquences  of  differences  between  functional  responses,  it \nwould still be redundant to test for those differences using \na null hypothesis framework. The null hypothesis is implic-\nitly posed as follows: ‘model results using each of the three \nfunctional  responses  are  drawn  from  populations  with \nidentical  distributions’.  Because  we  know  the  model  was \nprogrammed  with  different  functions  and  parameters  in \neach case, we also know a priori that the null hypothesis is \nfalse.  Thus  any  failure  to  reject  the  null  hypothesis  is  by \ndefinition  a  type-II  error,  and  the  test  merely  needs  suffi-\ncient power to detect an effect and avoid that error.\nThe  second  problem  with  hypothesis  tests  like  the  one \nMarzloff et al. (2013) perform is the arbitrariness of statisti-\ncal power itself in this setting. A sample size of 24,000 will \ngenerally  produce  a  significant  result  (p \f 0.05,  or  even \nmuch  lower)  regardless  of  the  magnitude  of  the  biological \neffect  size.  Indeed,  when  dealing  with  this  type  of  model, \none  can  literally  choose  the  desired  p-value  by  setting  the \nnumber of runs. With sufficient computer time, there is no \nlimit to how small a value can be obtained. This sample size \ninfluence  occurs,  of  course,  in  empirical  experiments,  not \njust  computer  simulations:  any  effect  size,  no  matter  how \nsmall,  can  be  found  significant  if  one  is  able  to  obtain \nenough  replicates  (Berkson  1938).  However  the  ease  and \nextremely low cost of replication in a simulation model set-\nting represents an absurd extreme for this general principle.\nMarzloff  et al.  (2013)  were  correct  to  disregard  the  low \np-values in their MANOVA, although we disagree with their \nchoice  to  fall  back  on  a  qualitative  visual  inspection  of  the \nmodel output. A better alternative would have been to directly \nquantify the effect size in the MANOVA (or simply compare \nthe distributions of community states; in their case a geomet-\nric comparison of the positions of model solutions in principal \ncomponents  space),  having  determined  beforehand  what \nmagnitude of difference (perhaps a percentage difference from \nsome baseline) would constitute a significant ecological effect \nin  the  study  system.  This  is  analogous  to  the  argument  that \necologists should focus on ‘biological significance’ rather than \nstatistical significance (Berkson 1942, Yoccoz 1991).\nEcological  simulation  models  are  not  the  only  context \nin which one could argue that the null hypothesis is known \nto be false a priori. Johnson (1999) catalogues several trivial \nnull hypotheses that were rejected in empirical field studies \n(e.g. that logged and unlogged forests have the same den-\nsity of trees). Johnson (1999) and others have argued that \nnull  hypotheses  are  usually  false  in  observational  studies, \nsuch  as  when  comparing  some  variable  (e.g.  fish  abun-\ndance) among two locations; it is unthinkable that the two \nlocations would have precisely identical population statis-\ntics. This situation stands in contrast to the case of a con-\ntrolled   ecological   experiment,   in   which   experimental \nreplicates are randomly assigned to treatments, and a null \nhypothesis  of  no  difference  among  the  treatment  popula-\ntions is actually a reasonable expectation (Johnson 1999). \nBecause simulation models are in a sense controlled numer-\nical  experiments  it  is  tempting  to  analyze  their  output  as \none  would  an  empirical  experiment.  However,  in  simula-\ntion models the relationship between treatment (parameter \nvalue,  functional  form)  and  some  dependent  response \n(model output) is explicit (mechanistic) and known (or at \nleast understood to exist); the question is simply how many \nreplicates are needed to detect that relationship statistically. \nIn  empirical  ecology,  we  accept  that  there  is  usually  some \ncorrespondence  between  statistical  significance  and  bio-\nlogical  significance  (i.e.  effect  size);  replication  is  difficult \nand costly to obtain, so power is a limiting factor and if a \nstatistically  significant  signal  is  detected,  it  is  likely  to  be \nbiologically meaningful as well. That correspondence breaks \ndown with the immense replication and power available in \na  simulation  model  setting.  Indeed,  it  is  these  features  of \nmodels  that  enable  their  analysis  to  bypass  significance-\ntests and focus on the key result of interest, effect size.\nAre peer reviewers the problem?\nWe  u s e d  Ma r z l o ff e t al. (2013) as an instructive example of \nthe folly of null hypothesis tests in part because the authors \n\n387\nthemselves admitted that the p-values they report are essen-\ntially meaningless. Indeed, the halfhearted use of MANOVA \nin their paper gives the sense of a test that was demanded \nby a reviewer during the peer review process (after all, if the \ntest results are meaningless, why bother conducting the test \nat  all,  or  taking  up  space  in  the  paper  to  report  it?).  We \nhave  encountered  similar  pressure  from  reviewers  in  our \nown work, and worry that it may be a common occurrence. \nA  non-systematic  search  quickly  revealed  many  recent \nexamples spanning the breadth of the ecological literature, \nfrom  terrestrial  plant  ecology  to  fisheries  to  life-history \ntheory (Marshall et al. 2008, Dauphin et al. 2009, Esther \net al. 2010, Makler-Pick et al. 2011, Tam and Ang 2012). \nWe suspect that calculating p-values for simulation model \nresults  is  actually  more  commonplace  than  a  literature \nsearch  would  reveal,  because  doubtless  there  are  many \ninstances of authors attempting this practice, but having it \nstripped  out  in  review  (which  all  of  the  authors  of  this \npaper have done as reviewer). We have also experienced the \nopposite  interaction,  in  which  reviewer  advocate  for  the \nnecessity of p-values. For example, Samhouri et al. (2009) \nanalyzed  seven  different  Ecopath  with  Ecosim  models  of \nmarine food web dynamics to identify system-specific indi-\ncators  (i.e.  biomass  of  a  particular  functional  group)  that \nreveal  changes  to  key  ecosystem  attributes  (i.e.  emergent \nproperties such as diversity, net primary productivity, mean \ntrophic level, etc.). For each model, they simulated a range \nof fishing perturbations to the model ecosystem, recorded \nthe  responses  of  a  suite  of  indicators  and  attributes,  and \ncalculated  the  correlations  between  each  indicator  and \nattribute. In their paper, they reported only the magnitude \nof  the  correlation  coefficients,  not  their  statistical  signifi-\ncance, noting that because of their large sample size, even \nsmall, biologically unmeaningful correlations would be sta-\ntistically significant (also see Fulton et al. 2005). This omis-\nsion was a point of contention during review, as a reviewer \nfocused on the potential for reporting spurious correlations \nin  the  absence  of  p-values.  Indeed,  there  are  plenty  of \nexamples in the literature in which p-values are reported to \nsupport or refute the use of indicators (Travers et al. 2006). \nSamhouri  et al.  (2009)  won  the  argument  with  their \nreviewer,  but  we  wonder  how  frequently  well-intentioned \nreviewers  –  particularly  those  with  more  experience  with \nempirical  data  than  models  –  insist  on  inappropriate \nhypothesis  tests  for  simulation  data.  The  importance  of \np-values and rigorous hypothesis tests is drilled into bud-\nding  ecologists  early  and  often,  so  the  mistake  is  under-\nstandable.  We  hope  that  this  paper  helps  formalize  the \nargument  for  modelers,  and  clarify  the  understanding  \nof  reviewers,  of  the  use  and  misuse  of  statistical  tests  in \nmodeling studies.\nIs there any use for frequentist statistics in ecological \nsimulation models?\nWe have argued that frequentists tests of null hypotheses are \nuseless when comparing model simulation output with dif-\nferent parameter values or functional forms. However, there \nare situations in which statistical tools associated  with fre-\nquentist  significance  testing  can  be  used  productively  in \nconcert  with  simulation  models.  For  example,  methods \nsuch  as  ANOVA  provide  a  useful  and  familiar  framework \nfor partitioning variance and calculating effect sizes in mul-\ntifactorial  simulations.  It  is  appropriate  to  use  ANOVA  in \nthat  way,  provided  one  ignores  (and  does  not  report)  the \np-values calculated along the way.\nCorell  et al.  (2012)  provide  a  representative  example  of \nthis usage. They examined the factors affecting dispersal dis-\ntances  of  planktonic  marine  larvae  in  the  Baltic  Sea.  They \nused   a   numerical   hydrodynamic   circulation   model   to \ndescribe  three-dimensional  flow  field  of  ocean  currents  in \nthe study region, and conducted simulations in which they \nreleased simulated Lagrangian particles (virtual ‘larvae’) into \nthe flow field and observed their trajectories. Larval trajecto-\nries  were  affected  by  random  turbulence,  so  the  dispersal \nmodel  includes  stochasticity.  The  authors  examined  the \neffects  of  multiple  factors  on  dispersal  trajectories  (e.g. \nspawning season, larval depth, duration of the larval period, \netc.)  and  created  a  factorial  design  to  examine  each  of  \n216 individual treatment combinations, with three replicate \nsimulations  per  combination.  They  then  analyzed  their \nresults using a five-way ANOVA. However, rather than use \nANOVA to test the hypothesis that different parameter val-\nues   produce   different   dispersal   patterns   (which   is   of  \ncourse  true),  they  focused  on  the  variance  components \nreturned  by  ANOVA  in  order  to  determine  which  factor \ncontributed  to  greater  absolute  variation  in  dispersal  dis-\ntance.  This  type  of  usage  is  perfectly  reasonable  (see  also  \nLegendre  and  De  Cáceres  2013).  Similar  applications  of  \nstatistical   methods   could   be,   for   example,   using   a  \nKomolgorov–Smirnov  test  to  compare  two  distributions \ngenerated by data; again, however, one should focus on the \neffect size or test statistic itself, rather than the p-value.\nAnother context in which hypothesis testing is usefully \napplied to simulation results is when one desires to simu-\nlate the empirical measurement of a system. This might be \ndone to test alternative statistical or experimental approaches \nin a system with known dynamics, or to determine how the \noutput  of  a  simulated  process  compares  to  observed  data. \nIn the first case, models are used to simulate both process \nand  measurement  error,  and  model  analysis  focuses  on \ndetermining  the  level  of  empirical  replication  needed  to \ndetect  a  process  (Hoban  et al.  2012)  or  validating  a  new \nstatistical method for detecting certain phenomena (Dakos \net al.  2012).  These  studies  are  essentially  statistical  power \nanalyses in which the known falsehood of the null hypoth-\nesis is taken as a given.\nAn  additional  case  where  statistics  could  appropriately \nbe applied to simulation outputs is when simulation results \nare being compared to observed empirical data. For exam-\nple, Walker and Cyr (2007) simulated neutral community \ndynamics  and  used  statistics  to  determine  whether  those \ndynamics  matched  observed  species  abundance  distribu-\ntions.  Similar  comparisons  are  common  in  time  series \napplications  where  model  forecast  skill  is  the  statistic  of \ninterest (Sugihara 1994). Although the details of compar-\ning simulated results to real data is a separate topic that is \nbeyond  the  scope  of  our  commentary,  we  note  that  stan-\ndard  frequentist  approaches  are  also  perilous  in  that  con-\ntext  (Waller  et al.  2003),  and  newer  methods  such  as \nApproximate   Bayesian   Computation   are   more   reliable \n(Hoban et al. 2012).\n\n388\nDauphin,  G.  et al.  2009.  Host  kairomone  learning  and  foraging \nsuccess  in  an  egg  parasitoid:  a  simulation  model.  –  Ecol. \nEntomol. 34: 193–203.\nEsther,  A.  et al.  2010.  Sensitivity  of  plant  functional  types  to \nclimate  change:  classification  tree  analysis  of  a  simulation \nmodel. – J. Veg. Sci. 21: 447–461.\nFulton, E. A. et al. 2005. Which ecological indicators can robustly \ndetect effects of fishing? – ICES J. Mar. Sci. 62: 540–551.\nHoban, S. et al. 2012. Computer simulations: tools for population \nand evolutionary genetics. – Nat. Rev. 13: 110–113.\nHurlbert,  S.  H.  and  Lombardi,  C.  M.  2009.  Final  collapse  \nof   the   Neyman–Pearson   decision   theoretic   framework  \nand  rise  of  the  neoFisherian.  –  Ann.  Zool.  Fenn.  46:  \n311–349.\nJohnson, D. H. 1999. The insignificance of statistical significance \ntesting. – J. Wildlife Manage. 63: 763–772.\nLegendre,  P.  and  De  Cáceres,  M.  2013.  Beta  diversity  as  the \nvariance  of  community  data:  dissimilarity  coefficients  and \npartitioning. – Ecol. Lett. 16: 951–963.\nMakler-Pick,  V.  et al.  2011.  Exploring  the  role  of  fish  in  a  lake \necosystem  (Lake  Kinneret,  Isreal)  by  coupling  an  individual-\nbased  fish  population  model  to  a  dynamic  ecosystem  model. \n– Can. J. Fish. Aquat. Sci. 68: 1265–1284.\nMarshall, D. J. et al. 2008. Offspring size variation within broods \nas  a  bet-hedging  strategy  in  unpredictable  environments.  \n– Ecology 89: 2506–2517.\nMarzloff, M. P. et al. 2013. Sensitivity analysis and pattern-oriented \nvalidation of TRITON, a model with alternative community \nstates:  insights  on  temperate  rocky  reefs  dynamics.  –  Ecol. \nModell. 258: 16–32.\nOaten,  A.  and  Murdoch,  W.  W.  1975a.  Functional  response  \nand  stability  in  predator–prey  systems.  –  Am.  Nat.  109:  \n289–298.\nOaten,  A.  and  Murdoch,  W.  W.  1975b.  Switching,  functional \nresponse,  and  stability  in  predator–prey  systems.  –  Am.  Nat. \n109: 299–318.\nSamhouri,  J.  F.  et al.  2009.  Quantitative  evaluation  of  marine \necosystem  indicator  performance  using  food  web  models.  \n– Ecosystems 12: 1283–1298.\nSavage, I. R. 1957. Nonparametric statistics. – J. Am. Stat. Ass. 52: \n331–344.\nSugihara,  G.  1994.  Nonlinear  forecasting  for  the  classification \nof  natural  time  series.  –  Phil.  Trans.  R.  Soc.  A  348:  \n477–495.\nTam, T. and Ang, P. O. 2012. Object-oriented simulation of coral \ncompetition in a coral reef community. – Ecol. Modell. 245: \n111–120.\nTr a v e r s ,   M .   e t al.  2006.  Simulating  and  testing  the  sensitivity \nof  ecosystem-based  indicators  to  fishing  in  the  southern \nBenguela   ecosystem.   –   Can.   J.   Fish.   Aquat.   Sci.   63:  \n943–956.\nWalker,  S.  C.  and  Cyr,  H.  2007.  Testing  the  standard  neutral \nmodel  of  biodiversity  in  lake  communities.  –  Oikos  116: \n143–155.\nWaller,  L.  A.  et al.  2003.  Monte  Carlo  assessments  of  goodness- \nof-fit  for  ecological  simulation  models.  –  Ecol.  Modell.  164: \n49–63.\nYoccoz, N. G. 1991. Use, overuse, and misuse of significance tests \nin  evolutionary  biology  and  ecology.  –  Bull.  Ecol.  Soc.  Am. \n72: 106–111.\nConclusion\nEcologists increasingly appreciate the importance of stochastic \nvariability, spatiotemporal heterogeneity, and complex non-\nlinearities to the dynamics of natural systems (Comins et al. \n1992,  Anderson  et al.  2008,  Berkley  et al.  2010).  This \n realization – coupled with the availability of fast computers \n–  is  motivating  increasing  reliance  upon  large  simulation \nmodels.  Here  we  have  explained  why  it  is  inappropriate  to \nrely upon frequentist statistical hypothesis tests designed for \nlow-replication   empirical   experiments   when   comparing \nhighly replicated outputs of simulation models. We have also \npointed out a few suitable applications of frequentist statis-\ntics to simulation model output, and there are surely others. \nNonetheless, we call upon authors (and reviewers) to avoid \nthe  temptation  to  analyze  model  output  with  a  focus  on \n statistical significance. The key insights to be to gleaned from \nsimulation models, as with empirical data, must come from \ninterpretation of biological significance.\nAcknowledgements – Authorship statement: all authors designed \nthe  study;  JWW  wrote  the  manuscript  with  input  from  all \nauthors.  AR,  JFS,  ACS  and  CW  contributed  equally  and  their \nauthor order is alphabetical. JWW was supported by Florida Sea \nGrant  PD/12-18.  ACS  was  supported  by  a  Killam  Fellowship. \nWe thank the Subject Editor for suggestions that improved this \nmanuscript. We also respectfully acknowledge the authors whose \nworks  we  singled  out  for  discussion.  We  thank  O.  Shelton,  \nM. Mangel and the Subject Editor for suggestions that improved \nthe manuscript.\nReferences\nAnderson, C. N. K. et al. 2008. Why fishing magnifies fluctuations \nin fish abundance. – Nature 452: 835–839.\nAnderson,  D.  R.  et al.  2000.  Null  hypothesis  testing:  problems, \nprevalence  and  an  alternative.  –  J.  Wildlife  Manage.  64:  \n912–923.\nBeninger, P. G. et al. 2012. Strengthening statistical usage in marine \necology. – J. Exp. Mar. Biol. Ecol. 426–427: 97–108.\nBerkley,  H.  A.  et al.  2010.  Turbulent  dispersal  promotes  species \ncoexistence. – Ecol. Lett. 13: 360–371.\nBerkson,  J.  1938.  Some  difficulties  of  interpretation  encountered \nin the application of the chi-square test. – J. Am. Stat. Ass. 33: \n526–542.\nBerkson,  J.  1942.  Tests  of  significance  considered  as  evidence.  \n– J. Am. Stat. Ass. 37: 325–335.\nComins, H. N. et al. 1992. The spatial dynamics of host–parasitoid \nsystems. – J. Anim. Ecol. 61: 735–748.\nCorell,  H.  et al.  2012.  Depth  distribution  of  larvae  critically  \naffects  their  dispersal  and  the  efficiency  of  marine  protected \nareas. – Mar. Ecol. Prog. Ser. 467: 29–46.\nDakos,  V.  et al.  2012.  Methods  for  detecting  early  warnings  of \ncritical  transitions  in  time  series  illustrated  using  simulated \necological data. – PLoS ONE 7:e41010.",
  "extractedAt": "2025-12-12T18:45:15.902Z",
  "success": true,
  "matchedPublication": {
    "title": "Ecologists should not use statistical significance tests to interpret simulation model results",
    "doi": "10.1111/j.1600-0706.2013.01073.x",
    "year": "2014",
    "authors": "White, J. Wilson; Rassweiler, Andrew; Samhouri, Jameal F.; Stier, Adrian C.; White, Crow"
  }
}